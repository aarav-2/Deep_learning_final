{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LSTM(with nulll) model, we observed a very high sMAPE value indicating that the model was not performing well. \n",
    "\n",
    "In this approach, we preprocess the data for each asset individually by extracting the 'high,' 'low,' 'close,' and 'volume' columns and scaling them using MinMaxScaler. We then create sliding windows over the scaled data for each asset, where n_timesteps of past data are used as inputs (X), and the following 10 timesteps are predicted as outputs (y). This ensures asset-specific feature scaling and consistent input-output preparation for forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 15:03:02.762783: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ExecutionTime</th>\n",
       "      <th>ID</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-06 21:45:00+01:00</td>\n",
       "      <td>Fri00Q1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-06 22:00:00+01:00</td>\n",
       "      <td>Fri00Q1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-06 22:15:00+01:00</td>\n",
       "      <td>Fri00Q1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-06 22:30:00+01:00</td>\n",
       "      <td>Fri00Q1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-06 22:45:00+01:00</td>\n",
       "      <td>Fri00Q1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ExecutionTime       ID  high  low  close  volume\n",
       "0  2021-01-06 21:45:00+01:00  Fri00Q1   0.0  0.0    0.0     0.0\n",
       "1  2021-01-06 22:00:00+01:00  Fri00Q1   0.0  0.0    0.0     0.0\n",
       "2  2021-01-06 22:15:00+01:00  Fri00Q1   0.0  0.0    0.0     0.0\n",
       "3  2021-01-06 22:30:00+01:00  Fri00Q1   0.0  0.0    0.0     0.0\n",
       "4  2021-01-06 22:45:00+01:00  Fri00Q1   0.0  0.0    0.0     0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training data\n",
    "train_df = pd.read_csv('TRAIN_Reco_2021_2022_2023.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "672"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the number of unique items in the 'ID' column\n",
    "unique_items_count = train_df['ID'].nunique()\n",
    "\n",
    "unique_items_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 69805344 rows\n",
      "Sampled dataset size: 13961069 rows\n",
      "Percentage of data retained after sampling: 20.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Define the forecasting horizon and look-back window\n",
    "n_timesteps = 10  # Look-back window\n",
    "forecast_horizon = 10  # Number of steps to predict\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Function to perform systematic sampling\n",
    "def systematic_sampling(data, reduction_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Perform systematic sampling to reduce the number of samples.\n",
    "    \n",
    "    Args:\n",
    "    data: The input data (NumPy array or DataFrame values).\n",
    "    reduction_ratio: The fraction of data to retain (e.g., 0.2 for 20% of the original data).\n",
    "    \n",
    "    Returns:\n",
    "    data_sampled: Systematically sampled input data.\n",
    "    \"\"\"\n",
    "    k = int(1 / reduction_ratio)\n",
    "    indices = np.arange(0, len(data), k)  # Systematically select every k-th sample\n",
    "    return data[indices]\n",
    "\n",
    "# Apply systematic sampling to the entire dataset before further processing\n",
    "train_df_sampled = systematic_sampling(train_df.values, reduction_ratio=0.2)  # Use `.values` to convert DataFrame to NumPy array\n",
    "\n",
    "# Check the size of the dataset after sampling\n",
    "print(f\"Original dataset size: {len(train_df)} rows\")\n",
    "print(f\"Sampled dataset size: {len(train_df_sampled)} rows\")\n",
    "print(f\"Percentage of data retained after sampling: {(len(train_df_sampled) / len(train_df)) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (13947629, 10, 4)\n",
      "Shape of y: (13947629, 10, 4)\n"
     ]
    }
   ],
   "source": [
    "# Convert train_df_sampled back to DataFrame if needed for further processing\n",
    "train_df_sampled = pd.DataFrame(train_df_sampled, columns=train_df.columns)\n",
    "\n",
    "# Now you can proceed with the loop code using the sampled data\n",
    "asset_ids = train_df_sampled['ID'].unique()  # Unique asset IDs from sampled data\n",
    "\n",
    "# Initialize empty lists to store inputs and outputs\n",
    "X, y = [], []\n",
    "\n",
    "# Loop through each asset and scale the features, then prepare sliding windows\n",
    "for asset in asset_ids:\n",
    "    # Extract the data for this asset\n",
    "    asset_data = train_df_sampled[train_df_sampled['ID'] == asset][['high', 'low', 'close', 'volume']].values\n",
    "    \n",
    "    # Scale the data using MinMaxScaler\n",
    "    asset_data_scaled = scaler.fit_transform(asset_data)  # Scaling for each asset separately\n",
    "\n",
    "    # Create sliding windows for the asset\n",
    "    for i in range(len(asset_data_scaled) - n_timesteps - forecast_horizon):\n",
    "        X.append(asset_data_scaled[i:i+n_timesteps])  # Past `n_timesteps` for input\n",
    "        y.append(asset_data_scaled[i+n_timesteps:i+n_timesteps+forecast_horizon])  # Next 10 timesteps for output\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_sampled = np.array(X)\n",
    "y_sampled = np.array(y)\n",
    "\n",
    "# Check the shapes of the processed data\n",
    "print(f\"Shape of X: {X_sampled.shape}\")\n",
    "print(f\"Shape of y: {y_sampled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced Shape of X_sampled: (8368577, 10, 4)\n",
      "Reduced Shape of y_sampled: (8368577, 10, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the reduction ratio (keeping 60% of the data)\n",
    "reduction_ratio = 0.6\n",
    "\n",
    "# Function to perform systematic sampling\n",
    "def systematic_sampling(data, reduction_ratio=0.6):\n",
    "    \"\"\"\n",
    "    Perform systematic sampling to reduce the number of samples.\n",
    "    \n",
    "    Args:\n",
    "    data: The input data (e.g., X_sampled or y_sampled).\n",
    "    reduction_ratio: The fraction of data to retain (e.g., 0.6 for 60% of the original data).\n",
    "    \n",
    "    Returns:\n",
    "    data_sampled: Systematically sampled input data.\n",
    "    \"\"\"\n",
    "    num_samples = int(len(data) * reduction_ratio)  # Calculate the exact number of samples to keep\n",
    "    indices = np.random.choice(len(data), size=num_samples, replace=False)  # Randomly select indices\n",
    "    return data[indices]\n",
    "\n",
    "# Apply systematic sampling to reduce the dataset by 40% (keeping 60%)\n",
    "X_sampled_new = systematic_sampling(X_sampled, reduction_ratio=0.6)\n",
    "y_sampled_new = systematic_sampling(y_sampled, reduction_ratio=0.6)\n",
    "\n",
    "# Check the shapes of the reduced dataset\n",
    "print(f\"Reduced Shape of X_sampled: {X_sampled_new.shape}\")\n",
    "print(f\"Reduced Shape of y_sampled: {y_sampled_new.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM - without active state as a feature**\n",
    "\n",
    "Step 1: Data Preparation\n",
    "\n",
    "1.1 Prepare Data for Scenario 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Creating Sliding Windows for All Assets\n",
    "\n",
    "Now, we will reshape the dataset to handle multiple assets together while preparing for LSTM input.\n",
    "\n",
    "Sliding Window: The sliding window approach means that instead of processing the entire dataset in one go, the model is trained on sequential windows of data. This reduces the load on memory and allows you to efficiently train on a smaller portion of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m209215/209215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2027s\u001b[0m 10ms/step - loss: 0.0149 - mae: 0.0714 - val_loss: 0.0148 - val_mae: 0.0720\n",
      "Epoch 2/10\n",
      "\u001b[1m209215/209215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2020s\u001b[0m 10ms/step - loss: 0.0149 - mae: 0.0713 - val_loss: 0.0149 - val_mae: 0.0697\n",
      "Epoch 3/10\n",
      "\u001b[1m209215/209215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1898s\u001b[0m 9ms/step - loss: 0.0148 - mae: 0.0713 - val_loss: 0.0149 - val_mae: 0.0692\n",
      "Epoch 4/10\n",
      "\u001b[1m209215/209215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1981s\u001b[0m 9ms/step - loss: 0.0148 - mae: 0.0713 - val_loss: 0.0148 - val_mae: 0.0707\n",
      "Epoch 5/10\n",
      "\u001b[1m209215/209215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1978s\u001b[0m 9ms/step - loss: 0.0148 - mae: 0.0713 - val_loss: 0.0148 - val_mae: 0.0717\n",
      "Epoch 6/10\n",
      "\u001b[1m209215/209215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1941s\u001b[0m 9ms/step - loss: 0.0148 - mae: 0.0713 - val_loss: 0.0148 - val_mae: 0.0724\n",
      "Epoch 7/10\n",
      "\u001b[1m209215/209215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2008s\u001b[0m 10ms/step - loss: 0.0148 - mae: 0.0713 - val_loss: 0.0148 - val_mae: 0.0721\n",
      "Epoch 8/10\n",
      "\u001b[1m209215/209215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2420s\u001b[0m 12ms/step - loss: 0.0148 - mae: 0.0713 - val_loss: 0.0148 - val_mae: 0.0701\n",
      "Epoch 9/10\n",
      "\u001b[1m209215/209215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2008s\u001b[0m 10ms/step - loss: 0.0148 - mae: 0.0713 - val_loss: 0.0148 - val_mae: 0.0710\n",
      "Epoch 10/10\n",
      "\u001b[1m209215/209215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2070s\u001b[0m 10ms/step - loss: 0.0149 - mae: 0.0713 - val_loss: 0.0148 - val_mae: 0.0708\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">68,096</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m68,096\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m4\u001b[0m)          │           \u001b[38;5;34m260\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">353,294</span> (1.35 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m353,294\u001b[0m (1.35 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">117,764</span> (460.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m117,764\u001b[0m (460.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">235,530</span> (920.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m235,530\u001b[0m (920.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, TimeDistributed, Input\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an explicit Input layer\n",
    "model.add(Input(shape=(n_timesteps, X_sampled_new.shape[2])))  # Input shape: (n_timesteps, number of features)\n",
    "\n",
    "# First LSTM layer with 128 units and return sequences\n",
    "model.add(LSTM(128, return_sequences=True))  # Returning the full sequence to the next LSTM layer\n",
    "model.add(Dropout(0.2))  # Dropout to prevent overfitting\n",
    "\n",
    "# Second LSTM layer with 64 units and return sequences (to match the time steps for forecasting)\n",
    "model.add(LSTM(64, return_sequences=True))  # Return full sequence (for the next 10 timesteps)\n",
    "\n",
    "# TimeDistributed Dense layer to output predictions for all features (HLCV) for all assets\n",
    "model.add(TimeDistributed(Dense(y_sampled_new.shape[2])))  # Predict the next 10 timesteps, with 4 features (HLCV)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_sampled_new, y_sampled_new, epochs=10, batch_size=32,validation_split=0.2)\n",
    "\n",
    "# Print model summary to see the architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the test dataset\n",
    "test_df = pd.read_csv('TEST_Reco_2024.csv')\n",
    "\n",
    "# Initialize the same scaler that was used for training\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Initialize empty lists for test data inputs and outputs\n",
    "X_test, y_test_true = [], []\n",
    "\n",
    "# Define the forecasting horizon and look-back window (same as training)\n",
    "n_timesteps = 10  # Look-back window\n",
    "forecast_horizon = 10  # Number of steps to predict\n",
    "\n",
    "# Get the list of unique asset IDs from the test data\n",
    "asset_ids_test = test_df['ID'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loop through each asset and scale the features, then prepare sliding windows\n",
    "for asset in asset_ids_test:\n",
    "    # Extract the data for this asset\n",
    "    asset_data = test_df[test_df['ID'] == asset][['high', 'low', 'close', 'volume']].values\n",
    "    \n",
    "    # Scale the data using the same MinMaxScaler used for training\n",
    "    asset_data_scaled = scaler.fit_transform(asset_data)\n",
    "\n",
    "    # Create sliding windows for the asset (same process as training)\n",
    "    for i in range(len(asset_data_scaled) - n_timesteps - forecast_horizon):\n",
    "        X_test.append(asset_data_scaled[i:i+n_timesteps])  # Past 10 timesteps for input\n",
    "        y_test_true.append(asset_data_scaled[i+n_timesteps:i+n_timesteps+forecast_horizon])  # Next 10 timesteps for output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m186463/457338\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m15:13\u001b[0m 3ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m457338/457338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1648s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy arrays\n",
    "X_test = np.array(X_test)\n",
    "y_test_true = np.array(y_test_true)\n",
    "\n",
    "# Make predictions using the trained LSTM model\n",
    "y_test_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_test: (14634816, 10, 4)\n",
      "Shape of y_test_true: (14634816, 10, 4)\n",
      "Shape of y_test_pred: (14634816, 10, 4)\n",
      "Shapes match, ready for evaluation!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reshape y_test_pred to match the shape of y_test_true\n",
    "# Note: y_test_pred may need reshaping because it's often output in a flat format\n",
    "y_test_pred = y_test_pred.reshape(y_test_true.shape[0], forecast_horizon, 4)  # Reshape to (samples, 10 timesteps, 4 features)\n",
    "\n",
    "# Check shapes\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(f\"Shape of y_test_true: {y_test_true.shape}\")\n",
    "print(f\"Shape of y_test_pred: {y_test_pred.shape}\")\n",
    "\n",
    "# Ensure the shapes are identical for comparison\n",
    "if y_test_true.shape == y_test_pred.shape:\n",
    "    print(\"Shapes match, ready for evaluation!\")\n",
    "else:\n",
    "    print(f\"Shapes do not match. y_test_true: {y_test_true.shape}, y_test_pred: {y_test_pred.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sMAPE score on the test set: 281.95349117883126%\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate Symmetric Mean Absolute Percentage Error (sMAPE)\n",
    "def smape(y_true, y_pred, epsilon=1e-10):\n",
    "    numerator = np.abs(y_true - y_pred)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    smape_val = np.mean(2 * numerator / (denominator + epsilon)) * 100  # Adding epsilon to avoid division by zero\n",
    "    return smape_val\n",
    "\n",
    "# Calculate sMAPE between true test values and predicted test values\n",
    "smape_score = smape(y_test_true, y_test_pred)\n",
    "\n",
    "# Print the sMAPE score\n",
    "print(f\"sMAPE score on the test set: {smape_score}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have a high sMAPE score but better than before. it indicates that the different approach to pre process the data was a good idea. \n",
    "The high score is likely due to the small scale of sub sample we took to train the data - 12%\n",
    "Even with such a small sample it took around 400 minutes to run the code on CPU. \n",
    "It shows the power of computational power and how because of it we are able to revolutionse the technology with AI. \n",
    "In the next model, we will remove the null values which is approximately 80% of the data and see if we can have a better score with the training on all the assets and a much larger and better distribution of the data "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
